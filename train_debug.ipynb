{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "import ruamel.yaml as yaml\n",
    "import torch\n",
    "import wandb\n",
    "import logging\n",
    "from logging import getLogger as get_logger\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from models import UNet, VAE, ClassEmbedder\n",
    "from schedulers import DDPMScheduler, DDIMScheduler\n",
    "from pipelines import DDPMPipeline\n",
    "from utils import (\n",
    "    seed_everything,\n",
    "    init_distributed_device,\n",
    "    is_primary,\n",
    "    AverageMeter,\n",
    "    str2bool,\n",
    "    save_checkpoint,\n",
    ")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"seed\": 42,\n",
    "    \"device\": device,\n",
    "    \n",
    "    \"image_size\": 128,\n",
    "    \"data_dir\": \"./data\",\n",
    "    \"subset\": 1.0,\n",
    "    \"batch_size\": 4,\n",
    "    \"num_workers\": 4,\n",
    "    \"output_dir\": \"experiments\",\n",
    "    \"run_name\": \"default\",\n",
    "\n",
    "    'use_cfg': False,\n",
    "\n",
    "    \"unet_in_size\": 128,\n",
    "    \"unet_in_ch\": 3,\n",
    "    \"unet_ch\": 128,\n",
    "    \"unet_num_res_blocks\": 2,\n",
    "    \"unet_ch_mult\": [1, 2, 2, 4],\n",
    "    \"unet_attn\": [2, 3],\n",
    "    \"unet_dropout\": 0.0,\n",
    "\n",
    "    \"num_train_timesteps\": 100,\n",
    "    \"num_inference_steps\": 100,\n",
    "    \"beta_start\": 0.0002,\n",
    "    \"beta_end\": 0.02,\n",
    "    \"beta_schedule\": \"linear\",\n",
    "    \"variance_type\": \"fixed_small\",\n",
    "    \"prediction_type\": \"epsilon\",\n",
    "    \"clip_sample\": True,\n",
    "    \"clip_sample_range\": 1.0,\n",
    "\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"num_epochs\": 10,\n",
    "\n",
    "    \"grad_clip\": 1.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(config[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(config[\"device\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((config[\"image_size\"], config[\"image_size\"])),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.CIFAR10(\n",
    "    config[\"data_dir\"], train=True, transform=transform, download=True\n",
    ")\n",
    "# train_dataset = datasets.ImageFolder(config[\"data_dir\"], transform=transform)\n",
    "# sorted_indices = sorted(range(len(train_dataset)),\n",
    "#                           key=lambda i: train_dataset.samples[i][0])\n",
    "# subset_size = int(len(train_dataset) * config[\"subset\"])\n",
    "# subset_indices = sorted_indices[:subset_size]\n",
    "# train_dataset = torch.utils.data.Subset(train_dataset, subset_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=config[\"num_workers\"],\n",
    "    pin_memory=True,\n",
    "    sampler=None,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "print(\"Image shape:\", images.shape)\n",
    "\n",
    "grid = make_grid(images, nrow=4, normalize=True)\n",
    "\n",
    "grid = (grid + 1.0) / 2.0\n",
    "grid = grid.clamp(0,1)\n",
    "grid = grid.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "plt.imshow(grid)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_batch_size = config[\"batch_size\"] * 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(config[\"output_dir\"]):\n",
    "    os.makedirs(config[\"output_dir\"])\n",
    "\n",
    "if config[\"run_name\"] is None:\n",
    "    config[\"run_name\"] = f\"exp-{len(os.listdir(config['output_dir']))}\"\n",
    "else:\n",
    "    config[\"run_name\"] = f\"exp-{len(os.listdir(config['output_dir']))}-{config['run_name']}\"\n",
    "output_dir = os.path.join(config[\"output_dir\"], config[\"run_name\"])\n",
    "save_dir = os.path.join(output_dir, \"checkpoints\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unet, DDPM Scheduler, Optimizer, LR Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet(\n",
    "    input_size=config[\"unet_in_size\"],\n",
    "    input_ch=config[\"unet_in_ch\"],\n",
    "    T=config[\"num_train_timesteps\"],\n",
    "    ch=config[\"unet_ch\"],\n",
    "    ch_mult=config[\"unet_ch_mult\"],\n",
    "    attn=config[\"unet_attn\"],\n",
    "    num_res_blocks=config[\"unet_num_res_blocks\"],\n",
    "    dropout=config[\"unet_dropout\"],\n",
    "    conditional=config[\"use_cfg\"],\n",
    "    c_dim=config[\"unet_ch\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = sum(p.numel() for p in unet.parameters() if p.requires_grad)\n",
    "print(f\"Number of parameters: {num_params / 10 ** 6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = unet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=config[\"num_train_timesteps\"],\n",
    "    num_inference_steps=config[\"num_inference_steps\"],\n",
    "    beta_start=config[\"beta_start\"],\n",
    "    beta_end=config[\"beta_end\"],\n",
    "    beta_schedule=config[\"beta_schedule\"],\n",
    "    variance_type=config[\"variance_type\"],\n",
    "    prediction_type=config[\"prediction_type\"],\n",
    "    clip_sample=config[\"clip_sample\"],\n",
    "    clip_sample_range=config[\"clip_sample_range\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = scheduler.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_wo_ddp = scheduler.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer, LR Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    unet.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"]\n",
    ")\n",
    "# TODO: setup scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=config[\"num_epochs\"] * len(train_loader), eta_min=1e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_update_steps_per_epoch = len(train_loader)\n",
    "config[\"max_train_steps\"] = config[\"num_epochs\"] * num_update_steps_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = DDPMPipeline(unet=unet, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"***** Training arguments *****\")\n",
    "print(\"***** Running training *****\")\n",
    "print(f\"  Num examples = {len(train_dataset)}\")\n",
    "print(f\"  Num Epochs = {config['num_epochs']}\")\n",
    "print(f\"  Instantaneous batch size per device = {config['batch_size']}\")\n",
    "print(\n",
    "    f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\"\n",
    ")\n",
    "print(\n",
    "    f\"  Total optimization steps per epoch {num_update_steps_per_epoch}\"\n",
    ")\n",
    "print(f\"  Total optimization steps = {config['max_train_steps']}\")\n",
    "\n",
    "progress_bar = tqdm(range(config[\"max_train_steps\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(config[\"num_epochs\"]):\n",
    "# for epoch in range(5):\n",
    "    loss_m = AverageMeter()\n",
    "\n",
    "    unet.train()\n",
    "    scheduler.train()\n",
    "\n",
    "    for step, (images, labels) in enumerate(train_loader):\n",
    "        batch_size = images.size(0)\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        noise = torch.randn_like(images)\n",
    "\n",
    "        timesteps = torch.randint(\n",
    "            0, scheduler.num_train_timesteps, (batch_size,), device=images.device\n",
    "        )\n",
    "\n",
    "        noisy_images = scheduler.add_noise(images, noise, timesteps)\n",
    "\n",
    "        # # visualize image\n",
    "        # grid = make_grid(images, nrow=4, normalize=True)\n",
    "        # grid = (grid + 1.0) / 2.0\n",
    "        # grid = grid.clamp(0,1)\n",
    "        # grid = grid.permute(1, 2, 0).cpu().numpy()\n",
    "        # plt.imshow(grid)\n",
    "        # plt.title(f\"Epoch {epoch}, Step {step}, Raw Image\")\n",
    "        # plt.show()\n",
    "\n",
    "        # grid = make_grid(noisy_images, nrow=4, normalize=True)\n",
    "        # grid = (grid + 1.0) / 2.0\n",
    "        # grid = grid.clamp(0,1)\n",
    "        # grid = grid.permute(1, 2, 0).cpu().numpy()\n",
    "        # plt.imshow(grid)\n",
    "        # plt.title(f\"Epoch {epoch}, Step {step}, Noise Image\")\n",
    "        # plt.show()\n",
    "\n",
    "        model_pred = unet(noisy_images, timesteps)\n",
    "\n",
    "        if config[\"prediction_type\"] == \"epsilon\":\n",
    "            target = noise\n",
    "\n",
    "        loss = F.mse_loss(model_pred, target)\n",
    "        loss_m.update(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        if config[\"grad_clip\"]:\n",
    "            torch.nn.utils.clip_grad_norm_(unet.parameters(), config[\"grad_clip\"])\n",
    "    \n",
    "        param_before = next(unet.parameters()).clone()\n",
    "        optimizer.step()\n",
    "        param_after = next(unet.parameters()).clone()\n",
    "        # print(f\"Parameter change: {torch.sum(torch.abs(param_after - param_before)).item()}\")\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        progress_bar.update(1)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}/{config['num_epochs']}, Step {step}/{num_update_steps_per_epoch}, Loss {loss.item()} ({loss_m.avg})\"\n",
    "            )\n",
    "        # break\n",
    "    \n",
    "    unet.eval()\n",
    "    generator = torch.Generator(device=device)\n",
    "    generator.manual_seed(epoch + config[\"seed\"])\n",
    "\n",
    "    gen_images = pipeline(\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        num_inference_steps=config[\"num_inference_steps\"],\n",
    "        generator=generator,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Convert PIL images to tensors\n",
    "    gen_images_tensors = [transforms.ToTensor()(img) for img in gen_images]\n",
    "\n",
    "    # Create a grid of images\n",
    "    grid = make_grid(gen_images_tensors, nrow=4, normalize=True)\n",
    "\n",
    "    # Convert the grid to a format suitable for display\n",
    "    grid = (grid + 1.0) / 2.0\n",
    "    grid = grid.clamp(0, 1)\n",
    "    grid = grid.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    # Display the grid\n",
    "    plt.imshow(grid)\n",
    "    plt.title(f\"Epoch {epoch}, Generated Image\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
