03/13/2025 21:42:03 - INFO - __main__ - ***** Training arguments *****
03/13/2025 21:42:03 - INFO - __main__ - Namespace(config='configs/ddpm.yaml', dataset='cifar10', data_dir='./data', image_size=128, batch_size=4, num_workers=4, num_classes=100, DEBUG=False, wandb_key='b6dff554fc854aea97af8dccb735c241a19a6615', project_name='dl-hw5', run_name='exp-11-test', output_dir='experiments', num_epochs=10, learning_rate=0.0001, weight_decay=0.0001, grad_clip=1.0, seed=42, mixed_precision='none', num_train_timesteps=1000, num_inference_steps=200, beta_start=0.0002, beta_end=0.02, beta_schedule='linear', variance_type='fixed_small', prediction_type='epsilon', clip_sample=True, clip_sample_range=1.0, unet_in_size=128, unet_in_ch=3, unet_ch=128, unet_ch_mult=[1, 2, 2, 4], unet_attn=[2, 3], unet_num_res_blocks=2, unet_dropout=0.0, latent_ddpm=False, use_cfg=False, cfg_guidance_scale=2.0, use_ddim=False, ckpt=None, predictor_type='epsilon', distributed=False, world_size=1, rank=0, local_rank=0, device='cuda', total_batch_size=4, max_train_steps=125000)
03/13/2025 21:42:03 - INFO - __main__ - ***** Running training *****
03/13/2025 21:42:03 - INFO - __main__ -   Num examples = 50000
03/13/2025 21:42:03 - INFO - __main__ -   Num Epochs = 10
03/13/2025 21:42:03 - INFO - __main__ -   Instantaneous batch size per device = 4
03/13/2025 21:42:03 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4
03/13/2025 21:42:03 - INFO - __main__ -   Total optimization steps per epoch 12500
03/13/2025 21:42:03 - INFO - __main__ -   Total optimization steps = 125000
  0%|                                                           | 0/125000 [00:00<?, ?it/s]03/13/2025 21:42:03 - INFO - __main__ - Epoch 1/10
  0%|                                                | 1/125000 [00:01<50:50:11,  1.46s/it]03/13/2025 21:42:05 - INFO - __main__ - Epoch 1/10, Step 0/12500, Loss 0.999896228313446 (0.999896228313446)
  0%|                                               | 101/125000 [00:18<6:03:04,  5.73it/s]03/13/2025 21:42:22 - INFO - __main__ - Epoch 1/10, Step 100/12500, Loss 0.3659548759460449 (0.7071317537586288)
  0%|                                               | 201/125000 [00:35<6:04:03,  5.71it/s]03/13/2025 21:42:39 - INFO - __main__ - Epoch 1/10, Step 200/12500, Loss 0.03143192082643509 (0.4355104145644909)
  0%|                                               | 301/125000 [00:53<6:01:24,  5.75it/s]03/13/2025 21:42:57 - INFO - __main__ - Epoch 1/10, Step 300/12500, Loss 0.012681338004767895 (0.29885613205115563)
  0%|▏                                              | 401/125000 [01:10<6:01:54,  5.74it/s]03/13/2025 21:43:14 - INFO - __main__ - Epoch 1/10, Step 400/12500, Loss 0.006006715353578329 (0.22738168647273577)
  0%|▏                                              | 501/125000 [01:28<6:01:46,  5.74it/s]03/13/2025 21:43:32 - INFO - __main__ - Epoch 1/10, Step 500/12500, Loss 0.0068014636635780334 (0.18433145026605718)
  0%|▏                                              | 558/125000 [01:38<6:01:21,  5.74it/s]Traceback (most recent call last):
  File "/root/hw5/train.py", line 541, in <module>
    main()
  File "/root/hw5/train.py", line 483, in main
    torch.nn.utils.clip_grad_norm_(unet.parameters(), args.grad_clip)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/clip_grad.py", line 39, in clip_grad_norm_
    grads = [p.grad for p in parameters if p.grad is not None]
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/clip_grad.py", line 39, in <listcomp>
    grads = [p.grad for p in parameters if p.grad is not None]
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 2192, in parameters
    for name, param in self.named_parameters(recurse=recurse):
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 2226, in named_parameters
    yield from gen
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 2160, in _named_members
    for module_prefix, module in modules:
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 2377, in named_modules
    yield from module.named_modules(memo, submodule_prefix, remove_duplicate)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 2377, in named_modules
    yield from module.named_modules(memo, submodule_prefix, remove_duplicate)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 2377, in named_modules
    yield from module.named_modules(memo, submodule_prefix, remove_duplicate)
  [Previous line repeated 1 more time]
KeyboardInterrupt
