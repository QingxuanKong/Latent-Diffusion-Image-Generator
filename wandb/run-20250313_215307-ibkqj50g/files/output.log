03/13/2025 21:53:07 - INFO - __main__ - ***** Training arguments *****
03/13/2025 21:53:07 - INFO - __main__ - Namespace(config='configs/ddpm.yaml', dataset='cifar10', data_dir='./data', image_size=128, batch_size=4, num_workers=4, num_classes=10, DEBUG=False, wandb_key='b6dff554fc854aea97af8dccb735c241a19a6615', project_name='dl-hw5', run_name='exp-16-test', output_dir='experiments', num_epochs=2, learning_rate=0.0001, weight_decay=0.0001, grad_clip=1.0, seed=42, mixed_precision='none', num_train_timesteps=100, num_inference_steps=50, beta_start=0.0002, beta_end=0.02, beta_schedule='linear', variance_type='fixed_small', prediction_type='epsilon', clip_sample=True, clip_sample_range=1.0, unet_in_size=128, unet_in_ch=3, unet_ch=128, unet_ch_mult=[1, 2, 2, 4], unet_attn=[2, 3], unet_num_res_blocks=2, unet_dropout=0.0, latent_ddpm=False, use_cfg=False, cfg_guidance_scale=2.0, use_ddim=False, ckpt=None, predictor_type='epsilon', distributed=False, world_size=1, rank=0, local_rank=0, device='cuda', total_batch_size=4, max_train_steps=2500)
03/13/2025 21:53:07 - INFO - __main__ - ***** Running training *****
03/13/2025 21:53:07 - INFO - __main__ -   Num examples = 5000
03/13/2025 21:53:07 - INFO - __main__ -   Num Epochs = 2
03/13/2025 21:53:07 - INFO - __main__ -   Instantaneous batch size per device = 4
03/13/2025 21:53:07 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4
03/13/2025 21:53:07 - INFO - __main__ -   Total optimization steps per epoch 1250
03/13/2025 21:53:07 - INFO - __main__ -   Total optimization steps = 2500
  0%|                                                             | 0/2500 [00:00<?, ?it/s]03/13/2025 21:53:07 - INFO - __main__ - Epoch 1/2
  0%|                                                     | 1/2500 [00:01<48:58,  1.18s/it]03/13/2025 21:53:09 - INFO - __main__ - Epoch 1/2, Step 0/1250, Loss 0.999896228313446 (0.999896228313446)
  4%|██                                                 | 101/2500 [00:18<06:51,  5.83it/s]03/13/2025 21:53:26 - INFO - __main__ - Epoch 1/2, Step 100/1250, Loss 0.3496880531311035 (0.7066938428595515)
  8%|████                                               | 201/2500 [00:35<06:35,  5.82it/s]03/13/2025 21:53:43 - INFO - __main__ - Epoch 1/2, Step 200/1250, Loss 0.0542207770049572 (0.4368851137583825)
 12%|██████▏                                            | 301/2500 [00:53<06:25,  5.71it/s]03/13/2025 21:54:00 - INFO - __main__ - Epoch 1/2, Step 300/1250, Loss 0.03720726817846298 (0.30418904099081245)
 16%|████████▏                                          | 401/2500 [01:10<06:02,  5.79it/s]03/13/2025 21:54:18 - INFO - __main__ - Epoch 1/2, Step 400/1250, Loss 0.028647731989622116 (0.23375044714315724)
 20%|██████████▏                                        | 501/2500 [01:27<05:48,  5.73it/s]03/13/2025 21:54:35 - INFO - __main__ - Epoch 1/2, Step 500/1250, Loss 0.012900680303573608 (0.19139109567893361)
 24%|████████████▎                                      | 601/2500 [01:45<05:29,  5.76it/s]03/13/2025 21:54:53 - INFO - __main__ - Epoch 1/2, Step 600/1250, Loss 0.03876130282878876 (0.1628142049281971)
 28%|██████████████▎                                    | 701/2500 [02:02<05:13,  5.75it/s]03/13/2025 21:55:10 - INFO - __main__ - Epoch 1/2, Step 700/1250, Loss 0.01900361105799675 (0.14234730535679724)
 32%|████████████████▎                                  | 801/2500 [02:20<04:54,  5.78it/s]03/13/2025 21:55:28 - INFO - __main__ - Epoch 1/2, Step 800/1250, Loss 0.015550060197710991 (0.12690987038692358)
 36%|██████████████████▍                                | 901/2500 [02:37<04:44,  5.62it/s]03/13/2025 21:55:45 - INFO - __main__ - Epoch 1/2, Step 900/1250, Loss 0.021313196048140526 (0.11489396886310885)
 40%|████████████████████                              | 1001/2500 [02:55<04:22,  5.72it/s]03/13/2025 21:56:03 - INFO - __main__ - Epoch 1/2, Step 1000/1250, Loss 0.020053833723068237 (0.10524579388816115)
 44%|██████████████████████                            | 1101/2500 [03:12<04:07,  5.65it/s]03/13/2025 21:56:20 - INFO - __main__ - Epoch 1/2, Step 1100/1250, Loss 0.012015027925372124 (0.09722940283417228)
 48%|████████████████████████                          | 1201/2500 [03:30<03:49,  5.66it/s]03/13/2025 21:56:38 - INFO - __main__ - Epoch 1/2, Step 1200/1250, Loss 0.013401798903942108 (0.09039286577990348)
 50%|█████████████████████████                         | 1250/2500 [03:38<03:36,  5.79it/s]Traceback (most recent call last):
  File "/root/hw5/train.py", line 544, in <module>
  File "/root/hw5/train.py", line 516, in main
    # create a blank canvas for the grid
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/hw5/pipelines/ddpm.py", line 98, in __call__
    image = randn_tensor(image_shape, generator=generator, device=device)
  File "/root/hw5/utils/misc.py", line 57, in randn_tensor
    latents = torch.randn(shape, generator=generator, device=rand_device, dtype=dtype, layout=layout).to(device)
TypeError: randn() received an invalid combination of arguments - got (tuple, layout=torch.layout, dtype=NoneType, device=torch.device, generator=NoneType), but expected one of:
 * (tuple of ints size, *, torch.Generator generator, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)
 * (tuple of ints size, *, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)
 * (tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)
 * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)
