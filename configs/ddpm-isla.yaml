seed: 42

DEBUG: False

# dataset
dataset: imagenet100
data_dir: /root/hw5/data
val_data_dir: /root/hw5/data/imagenet100_128x128/validation
subset: 1.0
image_size: 64
num_classes: 100

# training
batch_size: 32
num_workers: 4
num_epochs: 10

# optimizer
learning_rate: 2e-4
weight_decay: 1e-4

# Isla wandb
wandb_key: b6dff554fc854aea97af8dccb735c241a19a6615
project_name: dl-hw5
run_name: ddim-cfg

# resume wandb
resume: True
resume_checkpoint_path: "/root/hw5/experiments/exp-11-ddim-cfg/checkpoints/checkpoint_epoch_4.pth"
wandb_resume_id: "fzdz9ukx"

# # Ivy wandb
# wandb_key: dbcb5b22a7512074ad91148adc5794d1955289ad
# project_name: IDL-hw5-Ivy
# run_name: ddim-cfg-epoch5-Baseline

# # Bryan wandb
# wandb_key: 2e236a3f67726acb3dd528ffcda489a4f92f5457
# project_name: dl-hw5
# run_name: ddpm-epoch5-Baseline-Bryan-run1

# unet
unet_in_size: 64
unet_in_ch: 3
unet_ch: 128
unet_ch_mult: [1, 2, 2, 4]
unet_attn: [2, 3]
unet_num_res_blocks: 2
unet_dropout: 0.0
use_adagn_resblock: False
use_transformer_bottleneck: False
transformer_depth: 1
transformer_num_heads: 8

# # vae unet
# unet_in_size: 64 / 4 = 16
# unet_in_ch: 4
# unet_ch: 128
# unet_ch_mult: [1, 2, 2, 4]
# unet_attn: [1, 2]
# unet_num_res_blocks: 2
# unet_dropout: 0.0
# use_adagn_resblock: False
# use_transformer_bottleneck: True
# transformer_depth: 1
# transformer_num_heads: 8
# 0.1845 or 0.18215

# ddpm
num_train_timesteps: 1000
num_inference_steps: 1000
beta_start: 0.0002
beta_end: 0.02
beta_schedule: linear
variance_type: fixed_small
predictor_type: epsilon

#latent
latent_ddpm: False

# cfg
use_cfg: True
cfg_guidance_scale: 3.0
cond_drop_rate: 0.1

# ddim
use_ddim: True

# checkpoint
keep_last_n: 1 # including the last one
keep_last_model: True
keep_best_model: False

#evaluation during training
eval_during_train: True
eval_every_n_epoch: 5 # it takes 10 min to generate 500 images for evaluation each time
eval_samples: 500 # 500
eval_classes: 50 # 50

# inference
inference_samples: 5000
#
