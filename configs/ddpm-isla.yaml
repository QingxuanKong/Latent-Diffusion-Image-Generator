seed: 42

DEBUG: False
output_dir: /workspace/experiments

# dataset
dataset: imagenet100
data_dir: /workspace/data
val_data_dir: /workspace/data/imagenet100_128x128/validation
subset: 1.0
image_size: 128
num_classes: 100

# training
batch_size: 64
num_workers: 16
num_epochs: 300
# mixed_precision: bf16

# optimizer
warmup_epochs: 10
max_epochs: 300
learning_rate: 5e-5
weight_decay: 1e-4

# Isla wandb
wandb_username: islakong-carnegie-mellon-university
wandb_key: b6dff554fc854aea97af8dccb735c241a19a6615
project_name: dl-hw5
run_name: newvae-ddpm-vae-cfg-transformer-finetune

# # resume wandb
# resume: True
# wandb_resume_id: "dpuac806"
# resume_checkpoint_path: "/workspace/checkpoint_epoch_59.pth"

# # Ivy wandb
# wandb_key: dbcb5b22a7512074ad91148adc5794d1955289ad
# project_name: IDL-hw5-Ivy
# run_name: ddim-cfg-epoch5-Baseline

# # Bryan wandb
# wandb_key: 2e236a3f67726acb3dd528ffcda489a4f92f5457
# project_name: dl-hw5
# run_name: ddpm-epoch5-Baseline-Bryan-run1

# # unet
# unet_in_size: 64
# unet_in_ch: 3
# unet_ch: 128
# unet_ch_mult: [1, 2, 2, 4]
# unet_attn: [2, 3]
# unet_num_res_blocks: 2
# unet_dropout: 0.0
# use_adagn_resblock: False
# use_transformer_bottleneck: False
# transformer_depth: 1
# transformer_num_heads: 8

# vae unet
unet_in_size: 32
unet_in_ch: 3
unet_ch: 192
unet_ch_mult: [1, 2, 2, 4]
unet_attn: [2, 3]
unet_num_res_blocks: 3
unet_dropout: 0.1
use_adagn_resblock: False
use_transformer_bottleneck: True
transformer_depth: 4
transformer_num_heads: 8

vae_config:
  double_z: True
  z_channels: 3
  embed_dim: 3
  resolution: 256
  in_channels: 3
  out_ch: 3
  ch: 128
  ch_mult: [1, 2, 4]
  num_res_blocks: 2

# ddpm
num_train_timesteps: 1000
num_inference_steps: 1000
beta_start: 0.0002
beta_end: 0.02
beta_schedule: cosine
variance_type: fixed_small
predictor_type: epsilon

# latent
latent_ddpm: True
vae_ckpt: "/workspace/pretrained/Model.ckpt"
freeze_vae_epoch: 10 # since this epoch, index starting from 1
kl_beta: 0.0005
lpips_lambda: 1.0
vae_lambda: 0.05

# cfg
use_cfg: True
cfg_guidance_scale: 3.0
cond_drop_rate: 0.1

# ddim
use_ddim: False

# checkpoint
keep_last_n: 1 # including the last one
keep_last_model: False
keep_best_model: False

# evaluation during training
eval_during_train: True
eval_every_n_epoch: 10 # it takes 10 min to generate 500 images for evaluation each time
eval_samples: 500 # 500
eval_classes: 50 # 50

# inference
ckpt: "/workspace/checkpoint_epoch_99.pth"
inference_samples: 5000

