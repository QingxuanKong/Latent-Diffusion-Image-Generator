seed: 42

DEBUG: False
output_dir: /workspace/experiments/

# dataset
dataset: imagenet100
data_dir: /root/hw5/data
val_data_dir: /root/hw5/data/imagenet100_128x128/validation
subset: 1.0
image_size: 64
num_classes: 100

# training
batch_size: 64
num_workers: 6
num_epochs: 10

# optimizer
warmup_epochs: 5
max_epochs: 50
learning_rate: 1e-4
weight_decay: 1e-4

# Isla wandb
wandb_username: islakong-carnegie-mellon-university
wandb_key: b6dff554fc854aea97af8dccb735c241a19a6615
project_name: dl-hw5
run_name: unet(256,1-2-4-8,1-2,3),transformer*2

# # resume wandb
# resume: True
# wandb_resume_id: "qxjhivvj"
# resume_checkpoint_path: "/workspace/experiments/exp-8-ddpm(cosine)-ddim(500)-vae-cfg(transformer*2)-lr(1e-4)/checkpoints/checkpoint_epoch_0.pth"

# # Ivy wandb
# wandb_key: dbcb5b22a7512074ad91148adc5794d1955289ad
# project_name: IDL-hw5-Ivy
# run_name: ddim-cfg-epoch5-Baseline

# # Bryan wandb
# wandb_key: 2e236a3f67726acb3dd528ffcda489a4f92f5457
# project_name: dl-hw5
# run_name: ddpm-epoch5-Baseline-Bryan-run1

# # unet
# unet_in_size: 64
# unet_in_ch: 3
# unet_ch: 128
# unet_ch_mult: [1, 2, 2, 4]
# unet_attn: [2, 3]
# unet_num_res_blocks: 2
# unet_dropout: 0.0
# use_adagn_resblock: False
# use_transformer_bottleneck: False
# transformer_depth: 1
# transformer_num_heads: 8

# vae unet
unet_in_size: 16
unet_in_ch: 3
unet_ch: 256
unet_ch_mult: [1, 2, 4, 8]
unet_attn: [1, 2]
unet_num_res_blocks: 3
unet_dropout: 0.1
use_adagn_resblock: False
use_transformer_bottleneck: True
transformer_depth: 2
transformer_num_heads: 8

vae_config:
  double_z: True
  z_channels: 3
  embed_dim: 3
  resolution: 256
  in_channels: 3
  out_ch: 3
  ch: 128
  ch_mult: [1, 2, 4]
  num_res_blocks: 2


# ddpm
num_train_timesteps: 1000
num_inference_steps: 500
beta_start: 0.0002
beta_end: 0.02
beta_schedule: cosine
variance_type: fixed_small
predictor_type: epsilon

#latent
latent_ddpm: True

# cfg
use_cfg: True
cfg_guidance_scale: 3.0
cond_drop_rate: 0.1

# ddim
use_ddim: True

# checkpoint
keep_last_n: 1 # including the last one
keep_last_model: True
keep_best_model: False

#evaluation during training
eval_during_train: True
eval_every_n_epoch: 10 # it takes 10 min to generate 500 images for evaluation each time
eval_samples: 500 # 500
eval_classes: 50 # 50

# inference
inference_samples: 5000
#
